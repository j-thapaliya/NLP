# -*- coding: utf-8 -*-
"""Encoder_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSQTNfiTJ47xX46H9eYt3wHNhatFJap_
"""

!pip install evaluate

#Most pretrained models of huggingface can be found in transformers - AutoModelForSequenceClassification (for us), AutoModel (general)
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

import numpy as np
import evaluate, random, time
import torch
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# Reproducibility and GPU
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Using device: {device}")

#Loading AG News dataset from Huggingface
ds = load_dataset("ag_news")
train_texts, train_labels = ds["train"]["text"], ds["train"]["label"]
test_texts, test_labels   = ds["test"]["text"], ds["test"]["label"]
num_labels = len(set(train_labels))

print(f"AG News loaded → {len(train_texts)} train / {len(test_texts)} test / {num_labels} classes")

print("\n Preparing dataset for BERT...")

#Base refers to smaller Bert model which uses 12 encoders and allows 512 word inputs, uncased means casing doesn't matter
base_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

#Truncation removes the words if they exceed context size for model (512 for Bert base)
#padding max_length adds and extends all sequences' length to 128 words
def preprocess(batch):
    return base_tokenizer(batch["text"], truncation=True, padding="max_length", max_length=128)

#batch tokenization will be used to tokenize whole dataset
tokenized_ds = ds.map(preprocess, batched=True)
tokenized_ds = tokenized_ds.rename_column("label", "labels")

#Convert the data format to Pytorch tensor and leave the tokenized input_ids, attention_mask and labels which is direct input for BERT
tokenized_ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

#Function that dynamically creates a new Trainer pipeline for any dataset and BERT models
def run_bert_experiment(model_name, tokenized_ds, num_labels, seed=42, epochs=2):
    print(f"\nFine-tuning {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)

    metric_acc = evaluate.load("accuracy")
    metric_f1 = evaluate.load("f1")

    def compute_metrics(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        return {
            "accuracy": metric_acc.compute(predictions=preds, references=labels)["accuracy"],
            "f1_macro": metric_f1.compute(predictions=preds, references=labels, average="macro")["f1"]
        }

    args = TrainingArguments(
        output_dir=f"./bert_results/{model_name.replace('/', '_')}", #directory to save the model
        eval_strategy="epoch", #evaluate the model at the end of every epoch
        save_strategy="epoch", #save the model after every epoch
        learning_rate=2e-5, #Smaller since we are fine-tuning, otherwise will overfit easily
        per_device_train_batch_size=64,
        per_device_eval_batch_size=64, #Batch size of evaluation can be doubled since no gradients are calculated here
        num_train_epochs=epochs,
        weight_decay=0.01, #L2 regularization strength for better generalization
        logging_steps=200, #Logging the progress messages (loss, learning rate etc) every 200 steps
        seed=seed, #Seed for reproducibility
        fp16=torch.cuda.is_available(), #GPU if available
        report_to="none", #No need to log the data to huggingface's db for visualization etc
        load_best_model_at_end=True, #Load the best model
        metric_for_best_model="accuracy"
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_ds["train"],
        eval_dataset=tokenized_ds["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    #Save the training time for each model
    t0 = time.time()
    trainer.train()
    train_time = time.time() - t0

    results = trainer.evaluate()
    acc, f1 = results["eval_accuracy"], results["eval_f1_macro"]
    print(f"{model_name} done → Acc={acc:.4f}, F1={f1:.4f}, Time={train_time/60:.1f} min")

    # Define class labels (in the same order as label IDs)
    class_names = ['World', 'Sports', 'Business', 'Science']

    # Compute confusion matrix for report
    preds_output = trainer.predict(tokenized_ds["test"])
    preds = np.argmax(preds_output.predictions, axis=-1)

    cm = confusion_matrix(preds_output.label_ids, preds)

    # Display confusion matrix with class names
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap='Blues', values_format='d')
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted Classes")
    plt.ylabel("Actual Classes")
    plt.show()

    return {"Model": model_name, "Accuracy": acc, "F1_macro": f1, "Train_time_s": train_time}

#Models used
bert_models = ["bert-base-uncased", "distilbert-base-uncased"]
bert_results = []

#Function call for fine-tuning the model
for m in bert_models:
    result = run_bert_experiment(m, tokenized_ds, num_labels, seed=seed, epochs=4)
    bert_results.append(result)

results_df = pd.DataFrame(bert_results)

print("\nFinal Comparison:")
display(results_df)

plt.figure(figsize=(8,4))
plt.bar(results_df["Model"], results_df["Accuracy"], color="steelblue")
plt.title("Model Comparison (Accuracy)")
plt.ylabel("Accuracy")
plt.ylim(0,1)
plt.show()

